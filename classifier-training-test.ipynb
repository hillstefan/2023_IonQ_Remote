{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ce7ff8-1f2d-4445-9289-42e246bb2176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optax\n",
      "  Using cached optax-0.1.4-py3-none-any.whl (154 kB)\n",
      "Collecting absl-py>=0.7.1\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /opt/conda/lib/python3.9/site-packages (from optax) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.9/site-packages (from optax) (1.23.5)\n",
      "Collecting jaxlib>=0.1.37\n",
      "  Using cached jaxlib-0.4.2-cp39-cp39-manylinux2014_x86_64.whl (71.9 MB)\n",
      "Collecting jax>=0.1.55\n",
      "  Using cached jax-0.4.2-py3-none-any.whl\n",
      "Collecting chex>=0.1.5\n",
      "  Using cached chex-0.1.5-py3-none-any.whl (85 kB)\n",
      "Collecting toolz>=0.9.0\n",
      "  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "Collecting dm-tree>=0.1.5\n",
      "  Using cached dm_tree-0.1.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n",
      "Collecting opt-einsum\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/conda/lib/python3.9/site-packages (from jax>=0.1.55->optax) (1.10.0)\n",
      "Installing collected packages: dm-tree, toolz, opt-einsum, absl-py, jaxlib, jax, chex, optax\n",
      "Successfully installed absl-py-1.4.0 chex-0.1.5 dm-tree-0.1.8 jax-0.4.2 jaxlib-0.4.2 opt-einsum-3.3.0 optax-0.1.4 toolz-0.12.0\n"
     ]
    }
   ],
   "source": [
    "! pip install optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5efe7a26-9a14-41ee-8f8e-f8edf454fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "from utils.utils import simulate, histogram_to_category\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import the circuits\n",
    "from circuits.encoderFRQI import encode as frqi\n",
    "from circuits.encoderBASIC import encode as basic_encoder\n",
    "\n",
    "from circuits.weightsCircuit import encode as weight_layer\n",
    "\n",
    "def run_training(X, y, encoder_fn, classifier_fn, backend='qiskit'):\n",
    "    \n",
    "    n_layers = 2\n",
    "    batch_size = 32\n",
    "    epochs = 1_000\n",
    "    \n",
    "    def circuit_wrapper(x: jnp.ndarray, w: jnp.ndarray) -> jnp.ndarray:\n",
    "        print(type(w))\n",
    "        print(w)\n",
    "        print()\n",
    "        #print(np.array(w[:][0]))\n",
    "        circuit = encoder_fn(np.array(jnp.array(x[0])))\n",
    "        classifier = classifier_fn(w.values())\n",
    "        \n",
    "        nq1 = circuit.width()\n",
    "        nq2 = classifier.width()\n",
    "        \n",
    "        nq = max(nq1, nq2)\n",
    "        qc = qiskit.QuantumCircuit(nq)\n",
    "        qc.append(circuit.to_instruction(), list(range(nq1)))\n",
    "        qc.append(classifier.to_instruction(), list(range(nq2)))\n",
    "\n",
    "        histogram = simulate(qc)\n",
    "        \n",
    "        return histogram_to_category(histogram)\n",
    "\n",
    "    \n",
    "    def loss(params: optax.Params, batch: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
    "        predictions = circuit_wrapper(batch, params)\n",
    "        \n",
    "        y_pred = jax.nn.one_hot(y % 2, 2).astype(jnp.float32).reshape(len(labels), 2)\n",
    "        loss_value = optax.sigmoid_binary_cross_entropy(y_hat, labels).sum(axis=-1)\n",
    "\n",
    "        return loss_value.mean()\n",
    "\n",
    "    def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n",
    "        opt_state = optimizer.init(params)\n",
    "\n",
    "        #@jax.jit\n",
    "        def step(params, opt_state, batch, labels):\n",
    "            loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "            return params, opt_state, loss_value\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            batch_index = np.random.randint(0, len(y), (batch_size,))\n",
    "            x_train_batch = X[batch_index]\n",
    "            y_train_batch = y[batch_index]\n",
    "\n",
    "\n",
    "            params, opt_state, loss_value = step(params, opt_state, x_train_batch, y_train_batch)\n",
    "            losses.append(loss_value)\n",
    "            if i % 100 == 0:\n",
    "                print(f'step {i}, loss: {loss_value}')\n",
    "\n",
    "            return params, losses\n",
    "\n",
    "    \n",
    "    initial_params = {\n",
    "        'w': jax.random.normal(shape=[16, n_layers], key=jax.random.PRNGKey(0)),\n",
    "    }\n",
    "    \n",
    "    optimizer = optax.adam(learning_rate=1e-2)\n",
    "    \n",
    "    optimal_params, losses = fit(initial_params, optimizer)\n",
    "    \n",
    "    optimal_classifier = classifier_fn(optimal_params)\n",
    "    \n",
    "    return optimal_classifier, losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "567143d2-d4d9-4f26-9161-b877c1da529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'w': Traced<ConcreteArray([[-0.5338914   0.8417911 ]\n",
      " [ 0.8115571   0.05308708]\n",
      " [ 0.72478807 -0.5391156 ]\n",
      " [-0.21932149  0.5509203 ]\n",
      " [ 0.16972555  1.1971722 ]\n",
      " [-1.0609422   0.28213271]\n",
      " [-1.0543169   1.0187539 ]\n",
      " [-0.42167255 -2.5889838 ]\n",
      " [ 0.3031899  -0.7655693 ]\n",
      " [ 1.3062729  -0.7149365 ]\n",
      " [-0.18686387 -1.8082983 ]\n",
      " [-0.46174228  0.17252915]\n",
      " [ 0.43107846  0.2948003 ]\n",
      " [-0.8942256  -0.30150604]\n",
      " [ 0.27695706 -1.4905776 ]\n",
      " [-0.5799751   0.9487235 ]], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
      "  primal = Array([[-0.5338914 ,  0.8417911 ],\n",
      "       [ 0.8115571 ,  0.05308708],\n",
      "       [ 0.72478807, -0.5391156 ],\n",
      "       [-0.21932149,  0.5509203 ],\n",
      "       [ 0.16972555,  1.1971722 ],\n",
      "       [-1.0609422 ,  0.28213271],\n",
      "       [-1.0543169 ,  1.0187539 ],\n",
      "       [-0.42167255, -2.5889838 ],\n",
      "       [ 0.3031899 , -0.7655693 ],\n",
      "       [ 1.3062729 , -0.7149365 ],\n",
      "       [-0.18686387, -1.8082983 ],\n",
      "       [-0.46174228,  0.17252915],\n",
      "       [ 0.43107846,  0.2948003 ],\n",
      "       [-0.8942256 , -0.30150604],\n",
      "       [ 0.27695706, -1.4905776 ],\n",
      "       [-0.5799751 ,  0.9487235 ]], dtype=float32)\n",
      "  tangent = Traced<ShapedArray(float32[16,2])>with<JaxprTrace(level=1/0)> with\n",
      "    pval = (ShapedArray(float32[16,2]), None)\n",
      "    recipe = LambdaBinding()}\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict_values' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m y_reshape \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mone_hot(y \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(jnp\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      5\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y_reshape, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.33\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m circuit_weights, losses \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasic_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_layer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 85\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(X, y, encoder_fn, classifier_fn, backend)\u001b[0m\n\u001b[1;32m     79\u001b[0m initial_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m: jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m16\u001b[39m, n_layers], key\u001b[38;5;241m=\u001b[39mjax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)),\n\u001b[1;32m     81\u001b[0m }\n\u001b[1;32m     83\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m optimal_params, losses \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m optimal_classifier \u001b[38;5;241m=\u001b[39m classifier_fn(optimal_params)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimal_classifier, losses\n",
      "Cell \u001b[0;32mIn[38], line 71\u001b[0m, in \u001b[0;36mrun_training.<locals>.fit\u001b[0;34m(params, optimizer)\u001b[0m\n\u001b[1;32m     67\u001b[0m x_train_batch \u001b[38;5;241m=\u001b[39m X[batch_index]\n\u001b[1;32m     68\u001b[0m y_train_batch \u001b[38;5;241m=\u001b[39m y[batch_index]\n\u001b[0;32m---> 71\u001b[0m params, opt_state, loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss_value)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[38], line 59\u001b[0m, in \u001b[0;36mrun_training.<locals>.fit.<locals>.step\u001b[0;34m(params, opt_state, batch, labels)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(params, opt_state, batch, labels):\n\u001b[0;32m---> 59\u001b[0m     loss_value, grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state, params)\n\u001b[1;32m     61\u001b[0m     params \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[38], line 47\u001b[0m, in \u001b[0;36mrun_training.<locals>.loss\u001b[0;34m(params, batch, labels)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(params: optax\u001b[38;5;241m.\u001b[39mParams, batch: jnp\u001b[38;5;241m.\u001b[39mndarray, labels: jnp\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jnp\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 47\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mcircuit_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mone_hot(y \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(jnp\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     50\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39msigmoid_binary_cross_entropy(y_hat, labels)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 31\u001b[0m, in \u001b[0;36mrun_training.<locals>.circuit_wrapper\u001b[0;34m(x, w)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#print(np.array(w[:][0]))\u001b[39;00m\n\u001b[1;32m     30\u001b[0m circuit \u001b[38;5;241m=\u001b[39m encoder_fn(np\u001b[38;5;241m.\u001b[39marray(jnp\u001b[38;5;241m.\u001b[39marray(x[\u001b[38;5;241m0\u001b[39m])))\n\u001b[0;32m---> 31\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m nq1 \u001b[38;5;241m=\u001b[39m circuit\u001b[38;5;241m.\u001b[39mwidth()\n\u001b[1;32m     34\u001b[0m nq2 \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mwidth()\n",
      "File \u001b[0;32m~/2023_IonQ_Remote/circuits/weightsCircuit.py:16\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m     13\u001b[0m qc \u001b[38;5;241m=\u001b[39m QuantumCircuit(qubits)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(w)):\n\u001b[0;32m---> 16\u001b[0m     qc\u001b[38;5;241m.\u001b[39mry(pi\u001b[38;5;241m*\u001b[39m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m,qubits[i])\n\u001b[1;32m     18\u001b[0m qc\u001b[38;5;241m=\u001b[39mtranspile(qc)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qc\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_values' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.load('data/images.npy')\n",
    "y = np.load('data/labels.npy')\n",
    "y_reshape = jax.nn.one_hot(y % 2, 2).astype(jnp.float32).reshape(2000, 2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reshape, test_size=0.33, random_state=42)\n",
    "\n",
    "circuit_weights, losses = run_training(X_train, y_train, basic_encoder, weight_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44a1324-6975-4e98-beaa-6df8950453c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [Default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
