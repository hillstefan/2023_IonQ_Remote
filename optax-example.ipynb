{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ec84fe-1a8a-4aa7-8365-cd3eaf59889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optax in /opt/conda/lib/python3.9/site-packages (0.1.4)\n",
      "Requirement already satisfied: chex>=0.1.5 in /opt/conda/lib/python3.9/site-packages (from optax) (0.1.5)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /opt/conda/lib/python3.9/site-packages (from optax) (4.4.0)\n",
      "Requirement already satisfied: jax>=0.1.55 in /opt/conda/lib/python3.9/site-packages (from optax) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.9/site-packages (from optax) (1.23.5)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from optax) (1.4.0)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /opt/conda/lib/python3.9/site-packages (from optax) (0.4.2)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /opt/conda/lib/python3.9/site-packages (from chex>=0.1.5->optax) (0.1.8)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from chex>=0.1.5->optax) (0.12.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/conda/lib/python3.9/site-packages (from jax>=0.1.55->optax) (1.10.0)\n",
      "Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.9/site-packages (from jax>=0.1.55->optax) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8196a633-814e-4b7e-9dde-445b43a9f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "NUM_TRAIN_STEPS = 1_000\n",
    "RAW_TRAINING_DATA = np.random.randint(255, size=(NUM_TRAIN_STEPS, BATCH_SIZE, 1))\n",
    "\n",
    "TRAINING_DATA = np.unpackbits(RAW_TRAINING_DATA.astype(np.uint8), axis=-1)\n",
    "LABELS = jax.nn.one_hot(RAW_TRAINING_DATA % 2, 2).astype(jnp.float32).reshape(NUM_TRAIN_STEPS, BATCH_SIZE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c8992c19-7130-4592-8236-0b59bcd1b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params = {\n",
    "    'hidden': jax.random.normal(shape=[8, 32], key=jax.random.PRNGKey(0)),\n",
    "    'output': jax.random.normal(shape=[32, 2], key=jax.random.PRNGKey(1)),\n",
    "}\n",
    "\n",
    "\n",
    "def variational_circuit(x: jnp.ndarray, params: jnp.ndarray) -> jnp.ndarray:\n",
    "  # TODO: this has to be replaced with our circuit where x is fed into the encoder and params['w'] is fed into \n",
    "  x = jnp.dot(x, params['hidden'])\n",
    "  x = jax.nn.relu(x)\n",
    "  x = jnp.dot(x, params['output'])\n",
    "  # TODO: like that\n",
    "  #x_ = encoder(x)\n",
    "  #x_ = weights(x)\n",
    "  return x\n",
    "\n",
    "\n",
    "def loss(params: optax.Params, batch: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
    "  y_hat = variational_circuit(batch, params)\n",
    "\n",
    "  # optax also provides a number of common loss functions.\n",
    "  loss_value = optax.sigmoid_binary_cross_entropy(y_hat, labels).sum(axis=-1)\n",
    "\n",
    "  return loss_value.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6949c16e-67f2-4ad6-994f-295c6ca2da9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 8.737648963928223\n",
      "step 100, loss: 0.44012880325317383\n",
      "step 200, loss: 0.049903374165296555\n",
      "step 300, loss: 0.0755145326256752\n",
      "step 400, loss: 0.021333251148462296\n",
      "step 500, loss: 0.008120113052427769\n",
      "step 600, loss: 0.005172015633434057\n",
      "step 700, loss: 0.004648191854357719\n",
      "step 800, loss: 0.0007008477696217597\n",
      "step 900, loss: 0.003817687975242734\n"
     ]
    }
   ],
   "source": [
    "def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  @jax.jit\n",
    "  def step(params, opt_state, batch, labels):\n",
    "    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss_value\n",
    "\n",
    "  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n",
    "    #print(batch.shape)\n",
    "    #print(labels.shape)\n",
    "    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n",
    "    if i % 100 == 0:\n",
    "      print(f'step {i}, loss: {loss_value}')\n",
    "\n",
    "  return params\n",
    "\n",
    "# Finally, we can fit our parametrized function using the Adam optimizer\n",
    "# provided by optax.\n",
    "optimizer = optax.adam(learning_rate=1e-2)\n",
    "params = fit(initial_params, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e1d715-6889-4151-9c87-c7a8546cb142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "767c9695-926b-4228-a43b-be5f4cd65521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 1, 0, 0],\n",
       "        [1, 1, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 1, ..., 1, 1, 0],\n",
       "        [1, 0, 1, ..., 1, 1, 1],\n",
       "        [0, 0, 0, ..., 0, 0, 1]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 1, 1, 0],\n",
       "        [1, 0, 1, ..., 0, 0, 0]],\n",
       "\n",
       "       [[1, 1, 1, ..., 1, 1, 1],\n",
       "        [0, 0, 1, ..., 0, 1, 1],\n",
       "        [1, 0, 1, ..., 0, 1, 0],\n",
       "        [0, 0, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 0, 0, 1]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 1, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [1, 1, 0, ..., 1, 1, 1],\n",
       "        [1, 0, 0, ..., 0, 0, 1]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 1, 0],\n",
       "        [0, 1, 0, ..., 1, 0, 1],\n",
       "        [0, 0, 1, ..., 1, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 0, 1]],\n",
       "\n",
       "       [[0, 0, 0, ..., 1, 1, 0],\n",
       "        [1, 0, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 0, ..., 1, 0, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 0],\n",
       "        [1, 1, 0, ..., 0, 1, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d6accee-f541-4c58-ac01-0d7fe7becbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_DATA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "689384b2-55ea-40f9-9c2b-7fec671a0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/images.npy')\n",
    "y = np.load('data/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0a078e3-0d20-42cb-aa17-4d533272d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_hot = jax.nn.one_hot(y % 2, 2).astype(jnp.float32).reshape(2000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f0f5e87-1913-4fc1-b377-f1c7ed58adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77beb367-09c0-482c-9994-a20579f6e550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1340, 28, 28)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b606c584-027f-4cca-9033-75ed6a87bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res = X_train.reshape(1340, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a886deb4-89fd-42a6-9d4d-7a1ba9d6980c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1340, 784)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75900c30-64c8-4b04-a782-f853e29dfffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1340, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d70ca87e-1841-4c43-96c1-f79c8ed52362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "935c0537-4c08-40df-ab60-24c21133b508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5, 2)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5cf12c00-86ad-40e4-ac53-1fb264bc73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params = {\n",
    "    'hidden': jax.random.normal(shape=[784, 32], key=jax.random.PRNGKey(0)),\n",
    "    'output': jax.random.normal(shape=[32, 2], key=jax.random.PRNGKey(1)),\n",
    "}\n",
    "\n",
    "def loss(params: optax.Params, batch: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
    "  y_hat = variational_circuit(batch, params)\n",
    "\n",
    "  # optax also provides a number of common loss functions.\n",
    "  loss_value = optax.sigmoid_binary_cross_entropy(y_hat, labels).sum(axis=-1)\n",
    "\n",
    "  return loss_value.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b965fd8b-463a-4bf6-b9ce-ca9d3df19923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 1.4303110837936401\n",
      "step 100, loss: 0.4556594491004944\n",
      "step 200, loss: 1.4550915956497192\n",
      "step 300, loss: 0.47574564814567566\n",
      "step 400, loss: 0.38762328028678894\n",
      "step 500, loss: 0.03408501669764519\n",
      "step 600, loss: 0.7990520000457764\n",
      "step 700, loss: 0.6098849177360535\n",
      "step 800, loss: 0.09991126507520676\n",
      "step 900, loss: 0.07916977256536484\n"
     ]
    }
   ],
   "source": [
    "def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  @jax.jit\n",
    "  def step(params, opt_state, batch, labels):\n",
    "    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss_value\n",
    "\n",
    "  for i in range(NUM_TRAIN_STEPS):\n",
    "    batch_index = np.random.randint(0, 1340, (BATCH_SIZE,))\n",
    "    x_train_batch = X_train_res[batch_index]\n",
    "    y_train_batch = y_train[batch_index]\n",
    "    \n",
    "    #print(x_train_batch.shape)\n",
    "    #print(y_train_batch.shape)\n",
    "    \n",
    "    \n",
    "    params, opt_state, loss_value = step(params, opt_state, x_train_batch, y_train_batch)\n",
    "    if i % 100 == 0:\n",
    "      print(f'step {i}, loss: {loss_value}')\n",
    "\n",
    "  return params\n",
    "\n",
    "# Finally, we can fit our parametrized function using the Adam optimizer\n",
    "# provided by optax.\n",
    "optimizer = optax.adam(learning_rate=1e-2)\n",
    "params = fit(initial_params, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c6a6a-02e0-4698-9bbe-85de64b27512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [Default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
